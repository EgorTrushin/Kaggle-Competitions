{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973eac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from clrp_utils import (\n",
    "    seed_everything,\n",
    "    create_folds,\n",
    "    create_dataloaders,\n",
    "    train_fold,\n",
    "    oof_predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3545ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "BATCH_SIZE = 8\n",
    "VAL_BATCH_SIZE = 16\n",
    "LR = 2.0e-5\n",
    "LR_CLF = 5.0e-4\n",
    "WARMUP_RATIO = 0.1\n",
    "EPOCHS = 3\n",
    "SEED_VAL = 3\n",
    "VAL_STEP = [100, 50, 20]\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "WEIGHT_DECAY = 0.01\n",
    "WEIGHT_DECAY_CLF = 0.00\n",
    "NUM_FOLDS = 5\n",
    "FOLDS_RANDOM_STATE = 1325\n",
    "GRADIENT_CLIPPING = True\n",
    "TRAIN_CSV = \"~/datasets/commonlitreadabilityprize/train.csv\"\n",
    "\n",
    "model_cfg = {\n",
    "    \"model\": \"roberta-base\",\n",
    "    \"weights_dir\": \"\",\n",
    "    \"tokenizer\": \"roberta-base\",\n",
    "    \"max_len\": 256,\n",
    "    \"hidden_dropout_prob\": 0.0,\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eaccbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED_VAL)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_cfg[\"tokenizer\"])\n",
    "\n",
    "df = pd.read_csv(TRAIN_CSV)\n",
    "df = create_folds(df, num_splits=NUM_FOLDS, random_state=FOLDS_RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed7c3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 / 5   Epoch 1 / 3   Batch 100 / 284   Val_Loss: 0.6240   Best_Val_Loss: 0.6240\n",
      "Fold 1 / 5   Epoch 1 / 3   Batch 200 / 284   Val_Loss: 0.6485   Best_Val_Loss: 0.6240\n",
      "Fold 1 / 5   Epoch 1 / 3   Batch 284 / 284   Val_Loss: 0.5674   Best_Val_Loss: 0.5674\n",
      "  Average training loss: 0.7142\n",
      "  Best Val Loss: 0.5674\n",
      "  Training epoch took: 0:01:26\n",
      "\n",
      "Fold 1 / 5   Epoch 2 / 3   Batch  50 / 284   Val_Loss: 0.5253   Best_Val_Loss: 0.5253\n",
      "Fold 1 / 5   Epoch 2 / 3   Batch 100 / 284   Val_Loss: 0.5286   Best_Val_Loss: 0.5253\n",
      "Fold 1 / 5   Epoch 2 / 3   Batch 150 / 284   Val_Loss: 0.5619   Best_Val_Loss: 0.5253\n",
      "Fold 1 / 5   Epoch 2 / 3   Batch 200 / 284   Val_Loss: 0.5011   Best_Val_Loss: 0.5011\n",
      "Fold 1 / 5   Epoch 2 / 3   Batch 250 / 284   Val_Loss: 0.5417   Best_Val_Loss: 0.5011\n",
      "Fold 1 / 5   Epoch 2 / 3   Batch 284 / 284   Val_Loss: 0.4974   Best_Val_Loss: 0.4974\n",
      "  Average training loss: 0.4427\n",
      "  Best Val Loss: 0.4974\n",
      "  Training epoch took: 0:01:43\n",
      "\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch  20 / 284   Val_Loss: 0.4914   Best_Val_Loss: 0.4914\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch  40 / 284   Val_Loss: 0.4994   Best_Val_Loss: 0.4914\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch  60 / 284   Val_Loss: 0.4882   Best_Val_Loss: 0.4882\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch  80 / 284   Val_Loss: 0.4930   Best_Val_Loss: 0.4882\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch 100 / 284   Val_Loss: 0.4861   Best_Val_Loss: 0.4861\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch 120 / 284   Val_Loss: 0.5234   Best_Val_Loss: 0.4861\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch 140 / 284   Val_Loss: 0.5002   Best_Val_Loss: 0.4861\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch 160 / 284   Val_Loss: 0.4974   Best_Val_Loss: 0.4861\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch 180 / 284   Val_Loss: 0.4865   Best_Val_Loss: 0.4861\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch 200 / 284   Val_Loss: 0.5029   Best_Val_Loss: 0.4861\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch 220 / 284   Val_Loss: 0.4939   Best_Val_Loss: 0.4861\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch 240 / 284   Val_Loss: 0.4932   Best_Val_Loss: 0.4861\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch 260 / 284   Val_Loss: 0.4926   Best_Val_Loss: 0.4861\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch 280 / 284   Val_Loss: 0.4929   Best_Val_Loss: 0.4861\n",
      "Fold 1 / 5   Epoch 3 / 3   Batch 284 / 284   Val_Loss: 0.4929   Best_Val_Loss: 0.4861\n",
      "  Average training loss: 0.2996\n",
      "  Best Val Loss: 0.4861\n",
      "  Training epoch took: 0:02:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2 / 5   Epoch 1 / 3   Batch 100 / 284   Val_Loss: 1.1038   Best_Val_Loss: 1.1038\n",
      "Fold 2 / 5   Epoch 1 / 3   Batch 200 / 284   Val_Loss: 0.6133   Best_Val_Loss: 0.6133\n",
      "Fold 2 / 5   Epoch 1 / 3   Batch 284 / 284   Val_Loss: 0.5323   Best_Val_Loss: 0.5323\n",
      "  Average training loss: 0.7332\n",
      "  Best Val Loss: 0.5323\n",
      "  Training epoch took: 0:01:28\n",
      "\n",
      "Fold 2 / 5   Epoch 2 / 3   Batch  50 / 284   Val_Loss: 0.5477   Best_Val_Loss: 0.5323\n",
      "Fold 2 / 5   Epoch 2 / 3   Batch 100 / 284   Val_Loss: 0.5357   Best_Val_Loss: 0.5323\n",
      "Fold 2 / 5   Epoch 2 / 3   Batch 150 / 284   Val_Loss: 0.4878   Best_Val_Loss: 0.4878\n",
      "Fold 2 / 5   Epoch 2 / 3   Batch 200 / 284   Val_Loss: 0.4835   Best_Val_Loss: 0.4835\n",
      "Fold 2 / 5   Epoch 2 / 3   Batch 250 / 284   Val_Loss: 0.4977   Best_Val_Loss: 0.4835\n",
      "Fold 2 / 5   Epoch 2 / 3   Batch 284 / 284   Val_Loss: 0.4979   Best_Val_Loss: 0.4835\n",
      "  Average training loss: 0.4579\n",
      "  Best Val Loss: 0.4835\n",
      "  Training epoch took: 0:01:42\n",
      "\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch  20 / 284   Val_Loss: 0.4687   Best_Val_Loss: 0.4687\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch  40 / 284   Val_Loss: 0.4826   Best_Val_Loss: 0.4687\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch  60 / 284   Val_Loss: 0.4618   Best_Val_Loss: 0.4618\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch  80 / 284   Val_Loss: 0.4640   Best_Val_Loss: 0.4618\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch 100 / 284   Val_Loss: 0.4867   Best_Val_Loss: 0.4618\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch 120 / 284   Val_Loss: 0.4620   Best_Val_Loss: 0.4618\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch 140 / 284   Val_Loss: 0.4637   Best_Val_Loss: 0.4618\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch 160 / 284   Val_Loss: 0.4673   Best_Val_Loss: 0.4618\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch 180 / 284   Val_Loss: 0.4680   Best_Val_Loss: 0.4618\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch 200 / 284   Val_Loss: 0.4609   Best_Val_Loss: 0.4609\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch 220 / 284   Val_Loss: 0.4661   Best_Val_Loss: 0.4609\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch 240 / 284   Val_Loss: 0.4625   Best_Val_Loss: 0.4609\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch 260 / 284   Val_Loss: 0.4630   Best_Val_Loss: 0.4609\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch 280 / 284   Val_Loss: 0.4632   Best_Val_Loss: 0.4609\n",
      "Fold 2 / 5   Epoch 3 / 3   Batch 284 / 284   Val_Loss: 0.4632   Best_Val_Loss: 0.4609\n",
      "  Average training loss: 0.3035\n",
      "  Best Val Loss: 0.4609\n",
      "  Training epoch took: 0:02:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 3 / 5   Epoch 1 / 3   Batch 100 / 284   Val_Loss: 0.7001   Best_Val_Loss: 0.7001\n",
      "Fold 3 / 5   Epoch 1 / 3   Batch 200 / 284   Val_Loss: 0.5704   Best_Val_Loss: 0.5704\n",
      "Fold 3 / 5   Epoch 1 / 3   Batch 284 / 284   Val_Loss: 0.5168   Best_Val_Loss: 0.5168\n",
      "  Average training loss: 0.7043\n",
      "  Best Val Loss: 0.5168\n",
      "  Training epoch took: 0:01:28\n",
      "\n",
      "Fold 3 / 5   Epoch 2 / 3   Batch  50 / 284   Val_Loss: 0.5142   Best_Val_Loss: 0.5142\n",
      "Fold 3 / 5   Epoch 2 / 3   Batch 100 / 284   Val_Loss: 0.5862   Best_Val_Loss: 0.5142\n",
      "Fold 3 / 5   Epoch 2 / 3   Batch 150 / 284   Val_Loss: 0.5226   Best_Val_Loss: 0.5142\n",
      "Fold 3 / 5   Epoch 2 / 3   Batch 200 / 284   Val_Loss: 0.5667   Best_Val_Loss: 0.5142\n",
      "Fold 3 / 5   Epoch 2 / 3   Batch 250 / 284   Val_Loss: 0.5384   Best_Val_Loss: 0.5142\n",
      "Fold 3 / 5   Epoch 2 / 3   Batch 284 / 284   Val_Loss: 0.5097   Best_Val_Loss: 0.5097\n",
      "  Average training loss: 0.4427\n",
      "  Best Val Loss: 0.5097\n",
      "  Training epoch took: 0:01:42\n",
      "\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch  20 / 284   Val_Loss: 0.5097   Best_Val_Loss: 0.5097\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch  40 / 284   Val_Loss: 0.4901   Best_Val_Loss: 0.4901\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch  60 / 284   Val_Loss: 0.5025   Best_Val_Loss: 0.4901\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch  80 / 284   Val_Loss: 0.5137   Best_Val_Loss: 0.4901\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch 100 / 284   Val_Loss: 0.4958   Best_Val_Loss: 0.4901\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch 120 / 284   Val_Loss: 0.4877   Best_Val_Loss: 0.4877\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch 140 / 284   Val_Loss: 0.4959   Best_Val_Loss: 0.4877\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch 160 / 284   Val_Loss: 0.4889   Best_Val_Loss: 0.4877\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch 180 / 284   Val_Loss: 0.4959   Best_Val_Loss: 0.4877\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch 200 / 284   Val_Loss: 0.4974   Best_Val_Loss: 0.4877\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch 220 / 284   Val_Loss: 0.5084   Best_Val_Loss: 0.4877\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch 240 / 284   Val_Loss: 0.4980   Best_Val_Loss: 0.4877\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch 260 / 284   Val_Loss: 0.4985   Best_Val_Loss: 0.4877\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch 280 / 284   Val_Loss: 0.4983   Best_Val_Loss: 0.4877\n",
      "Fold 3 / 5   Epoch 3 / 3   Batch 284 / 284   Val_Loss: 0.4983   Best_Val_Loss: 0.4877\n",
      "  Average training loss: 0.2968\n",
      "  Best Val Loss: 0.4877\n",
      "  Training epoch took: 0:02:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 4 / 5   Epoch 1 / 3   Batch 100 / 284   Val_Loss: 0.6797   Best_Val_Loss: 0.6797\n",
      "Fold 4 / 5   Epoch 1 / 3   Batch 200 / 284   Val_Loss: 0.6709   Best_Val_Loss: 0.6709\n",
      "Fold 4 / 5   Epoch 1 / 3   Batch 284 / 284   Val_Loss: 0.6214   Best_Val_Loss: 0.6214\n",
      "  Average training loss: 0.7064\n",
      "  Best Val Loss: 0.6214\n",
      "  Training epoch took: 0:01:28\n",
      "\n",
      "Fold 4 / 5   Epoch 2 / 3   Batch  50 / 284   Val_Loss: 0.5524   Best_Val_Loss: 0.5524\n",
      "Fold 4 / 5   Epoch 2 / 3   Batch 100 / 284   Val_Loss: 0.4915   Best_Val_Loss: 0.4915\n",
      "Fold 4 / 5   Epoch 2 / 3   Batch 150 / 284   Val_Loss: 0.4961   Best_Val_Loss: 0.4915\n",
      "Fold 4 / 5   Epoch 2 / 3   Batch 200 / 284   Val_Loss: 0.4834   Best_Val_Loss: 0.4834\n",
      "Fold 4 / 5   Epoch 2 / 3   Batch 250 / 284   Val_Loss: 0.4954   Best_Val_Loss: 0.4834\n",
      "Fold 4 / 5   Epoch 2 / 3   Batch 284 / 284   Val_Loss: 0.5231   Best_Val_Loss: 0.4834\n",
      "  Average training loss: 0.4585\n",
      "  Best Val Loss: 0.4834\n",
      "  Training epoch took: 0:01:43\n",
      "\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch  20 / 284   Val_Loss: 0.4816   Best_Val_Loss: 0.4816\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch  40 / 284   Val_Loss: 0.5178   Best_Val_Loss: 0.4816\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch  60 / 284   Val_Loss: 0.4694   Best_Val_Loss: 0.4694\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch  80 / 284   Val_Loss: 0.5141   Best_Val_Loss: 0.4694\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch 100 / 284   Val_Loss: 0.4719   Best_Val_Loss: 0.4694\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch 120 / 284   Val_Loss: 0.4754   Best_Val_Loss: 0.4694\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch 140 / 284   Val_Loss: 0.4634   Best_Val_Loss: 0.4634\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch 160 / 284   Val_Loss: 0.4651   Best_Val_Loss: 0.4634\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch 180 / 284   Val_Loss: 0.4722   Best_Val_Loss: 0.4634\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch 200 / 284   Val_Loss: 0.4610   Best_Val_Loss: 0.4610\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch 220 / 284   Val_Loss: 0.4617   Best_Val_Loss: 0.4610\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch 240 / 284   Val_Loss: 0.4618   Best_Val_Loss: 0.4610\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch 260 / 284   Val_Loss: 0.4631   Best_Val_Loss: 0.4610\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch 280 / 284   Val_Loss: 0.4631   Best_Val_Loss: 0.4610\n",
      "Fold 4 / 5   Epoch 3 / 3   Batch 284 / 284   Val_Loss: 0.4631   Best_Val_Loss: 0.4610\n",
      "  Average training loss: 0.3244\n",
      "  Best Val Loss: 0.4610\n",
      "  Training epoch took: 0:02:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5 / 5   Epoch 1 / 3   Batch 100 / 284   Val_Loss: 0.7368   Best_Val_Loss: 0.7368\n",
      "Fold 5 / 5   Epoch 1 / 3   Batch 200 / 284   Val_Loss: 0.6031   Best_Val_Loss: 0.6031\n",
      "Fold 5 / 5   Epoch 1 / 3   Batch 284 / 284   Val_Loss: 0.5366   Best_Val_Loss: 0.5366\n",
      "  Average training loss: 0.6957\n",
      "  Best Val Loss: 0.5366\n",
      "  Training epoch took: 0:01:28\n",
      "\n",
      "Fold 5 / 5   Epoch 2 / 3   Batch  50 / 284   Val_Loss: 0.5801   Best_Val_Loss: 0.5366\n",
      "Fold 5 / 5   Epoch 2 / 3   Batch 100 / 284   Val_Loss: 0.5208   Best_Val_Loss: 0.5208\n",
      "Fold 5 / 5   Epoch 2 / 3   Batch 150 / 284   Val_Loss: 0.5146   Best_Val_Loss: 0.5146\n",
      "Fold 5 / 5   Epoch 2 / 3   Batch 200 / 284   Val_Loss: 0.4928   Best_Val_Loss: 0.4928\n",
      "Fold 5 / 5   Epoch 2 / 3   Batch 250 / 284   Val_Loss: 0.4898   Best_Val_Loss: 0.4898\n",
      "Fold 5 / 5   Epoch 2 / 3   Batch 284 / 284   Val_Loss: 0.4980   Best_Val_Loss: 0.4898\n",
      "  Average training loss: 0.4493\n",
      "  Best Val Loss: 0.4898\n",
      "  Training epoch took: 0:01:43\n",
      "\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch  20 / 284   Val_Loss: 0.4753   Best_Val_Loss: 0.4753\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch  40 / 284   Val_Loss: 0.4879   Best_Val_Loss: 0.4753\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch  60 / 284   Val_Loss: 0.4771   Best_Val_Loss: 0.4753\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch  80 / 284   Val_Loss: 0.4857   Best_Val_Loss: 0.4753\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch 100 / 284   Val_Loss: 0.4865   Best_Val_Loss: 0.4753\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch 120 / 284   Val_Loss: 0.4712   Best_Val_Loss: 0.4712\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch 140 / 284   Val_Loss: 0.4725   Best_Val_Loss: 0.4712\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch 160 / 284   Val_Loss: 0.4789   Best_Val_Loss: 0.4712\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch 180 / 284   Val_Loss: 0.4688   Best_Val_Loss: 0.4688\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch 200 / 284   Val_Loss: 0.4730   Best_Val_Loss: 0.4688\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch 220 / 284   Val_Loss: 0.4761   Best_Val_Loss: 0.4688\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch 240 / 284   Val_Loss: 0.4714   Best_Val_Loss: 0.4688\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch 260 / 284   Val_Loss: 0.4711   Best_Val_Loss: 0.4688\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch 280 / 284   Val_Loss: 0.4712   Best_Val_Loss: 0.4688\n",
      "Fold 5 / 5   Epoch 3 / 3   Batch 284 / 284   Val_Loss: 0.4712   Best_Val_Loss: 0.4688\n",
      "  Average training loss: 0.3168\n",
      "  Best Val Loss: 0.4688\n",
      "  Training epoch took: 0:02:26\n"
     ]
    }
   ],
   "source": [
    "best_val_losses = list()\n",
    "\n",
    "for fold in range(NUM_FOLDS):\n",
    "\n",
    "    train_set, valid_set = df[df[\"kfold\"] != fold], df[df[\"kfold\"] == fold]\n",
    "\n",
    "    train_dataloader, validation_dataloader = create_dataloaders(\n",
    "        tokenizer,\n",
    "        train_set,\n",
    "        valid_set=valid_set,\n",
    "        max_len=model_cfg[\"max_len\"],\n",
    "        train_batch_size=BATCH_SIZE,\n",
    "        valid_batch_size=VAL_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    if \"bert\" in model_cfg[\"model\"]:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_cfg[\"model\"],\n",
    "            num_labels=1,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            hidden_dropout_prob=model_cfg[\"hidden_dropout_prob\"],\n",
    "            attention_probs_dropout_prob=model_cfg[\"attention_probs_dropout_prob\"],\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_cfg[\"model\"],\n",
    "            num_labels=1,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            dropout=model_cfg[\"droput\"],\n",
    "            summary_last_dropout=model_cfg[\"summary_last_dropout\"],\n",
    "        )\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    classifier = [\"classifier\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if \"classifier\" not in n],\n",
    "            \"lr\": LR,\n",
    "            \"weight_decay_rate\": WEIGHT_DECAY,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if \"classifier\" in n],\n",
    "            \"lr\": LR_CLF,\n",
    "            \"weight_decay_rate\": WEIGHT_DECAY_CLF,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        optimizer_parameters,\n",
    "        lr=LR,\n",
    "        betas=(0.9, 0.98),\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        eps=1e-6,\n",
    "        correct_bias=False,\n",
    "    )\n",
    "\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=WARMUP_RATIO * total_steps,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    best_val_loss = train_fold(\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        train_dataloader,\n",
    "        validation_dataloader,\n",
    "        DEVICE,\n",
    "        fold,\n",
    "        model_cfg[\"model\"],\n",
    "        epochs=EPOCHS,\n",
    "        val_step=VAL_STEP,\n",
    "        num_folds=NUM_FOLDS,\n",
    "        gradient_clipping=GRADIENT_CLIPPING,\n",
    "    )\n",
    "    best_val_losses.append(best_val_loss)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    del train_dataloader, validation_dataloader, model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e75bc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Val Losses:\n",
      "Fold: 0   Loss: 0.48605\n",
      "Fold: 1   Loss: 0.46094\n",
      "Fold: 2   Loss: 0.48767\n",
      "Fold: 3   Loss: 0.46103\n",
      "Fold: 4   Loss: 0.46878\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBest Val Losses:\")\n",
    "for i, loss in enumerate(best_val_losses):\n",
    "    print(\"Fold: {:}   Loss: {:.5f}\".format(i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e07f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc69abb0da54ce181f0a6a3ce00cdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model 0:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39af076d8ac6434a90276d473fd5770f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model 1:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8a06c2dc004b99a39174a4bc6785e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model 2:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5575f697004efd99aed132e8736114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model 3:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c699c4a0d56451292b90cd140c7a210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model 4:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score = 0.47956\n"
     ]
    }
   ],
   "source": [
    "oof_preds = oof_predictions(df, model_cfg, DEVICE)\n",
    "\n",
    "oof_combined = np.zeros(len(df))\n",
    "for fold in oof_preds:\n",
    "    oof_combined[oof_preds[fold][\"val_index\"]] += oof_preds[fold][\"preds\"]\n",
    "\n",
    "cv_score = np.sqrt(mean_squared_error(df.target.values, oof_combined))\n",
    "print(\"CV score = {:.5f}\".format(cv_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
